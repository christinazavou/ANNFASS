
###GAN architecture

Consists of a generator and a discriminator model.

##### Generator
Responsible for creating new outputs, such as images, that plausibly could have come from the original dataset.

Typically implemented using a deep convolutional neural network and results-specialized layers that learn to fill in features in an image rather than extract features from an input image.

Two common types of layers that can be used in the generator model are a upsample layer (in keras: UpSampling2D) that simply doubles the dimensions of the input and the transpose convolutional layer (in keras: Conv2DTranspose) that performs an inverse convolution operation.

Generator needs a layer to translate from coarse salient features to a more dense and detailed output.

##### Discriminator
Responsible for classifying images as either real (from the domain) or fake (generated).


Training Pseudocode:

```python
# train the discriminator to classify a batch of images from our dataset as real and a 
# batch of images generated by our current generator as fake
1.discriminator.train_on_batch(image_batch=real_image_batch, labels=real)
2.discriminator.train_on_batch(image_batch=generated_image_batch, labels=fake)

# train the generator to trick the discriminator into classifying a batch of generated images as real.
# The key here is that the discriminator is frozen (not trainable) in this step, 
# but it's loss functions gradients are back-propagated through the combined network to the generator.
# The generator updates its weights in the most ideal way possible based on these gradients
3.combined.train_on_batch(input=batch_of_noise, labels=real)
# where combined is a model that consists of the generator and discriminator joined together such that: 
# input => generator => generator_output => discriminator => classification
```

[** cool example with music and deaf people](https://www.kdnuggets.com/2017/03/deep-learning-gans-boxing-fundamental-understanding.html)

notes:
- only the discriminator has direct access to the data!
- After training a GAN, most current methods use the discriminator as a base model for transfer learning and then fine-tuning for a production model, or the generator as a source of data that is used to train a production model. 

#### most common fail cases of GANs:
1. The discriminator becomes too strong too quickly and the generator ends up not learning anything. (i.e. in step 3. the discriminator classifies generated data as fake so accurately and confidently that there is nothing in the discriminator’s back-propagated loss function gradients for the generator to learn.)
2. The generator only learns very specific weaknesses of the discriminator and takes advantage of these to trick the discriminator into classifying generated data as real instead of learning to represent the true data distribution.
3. The generator learns only a very small subset of the true data distribution. (thus the discriminator doesnt learn anything new).  An example of this occurring in practice is the case where for every possible input, the generator is generating the same data sample and there is no variation among it’s outputs.

### My Questions:
1. ~~what is batch of noise ?~~i think it's really just dummy input i.e. noise images that will pass through generator and he will output some new images..and if they dont look like the real images given to discriminator the generator will need to adapt his weights a lot ... when he generates out of noise images that look like the real ones he is finally trained :) ~~Also i guess we do repeat 1. and 2. so that whole system will not just learn to generate noise and classify it as fake. i.e. loss of 1. 2. and 3. are all important~~ indeed
